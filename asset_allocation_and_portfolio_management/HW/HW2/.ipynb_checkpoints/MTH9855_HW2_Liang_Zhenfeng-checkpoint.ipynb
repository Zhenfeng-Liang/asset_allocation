{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zhenfeng Liang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is convex IFF it satisfy,\n",
    "\n",
    "$$f(\\theta x + (1 - \\theta)y) \\leq \\theta f(x) + (1 - \\theta)f(y)$$\n",
    "\n",
    "Apparently, when x and y have the same sign, the inequality holds.\n",
    "\n",
    "Let $x$ and $y$ be two variables such that, $x < 0 \\leq y$.\n",
    "\n",
    "When $\\theta x + (1 - \\theta)y < 0$, \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "&&\\phi(\\theta x + (1 - \\theta)y) \\leq \\theta \\phi(x) + (1 - \\theta)\\phi(y) \\Rightarrow l(\\theta x + (1 - \\theta)y) \\leq l\\theta x + r(1 - \\theta)y\\\\\n",
    "&\\Rightarrow& l(1 - \\theta)y \\leq r(1 - \\theta)y \\Rightarrow l \\leq r \n",
    "\\end{eqnarray*}\n",
    "\n",
    "When $\\theta x + (1 - \\theta)y \\geq 0$, \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "&&\\phi(\\theta x + (1 - \\theta)y) \\leq \\theta \\phi(x) + (1 - \\theta)\\phi(y) \\Rightarrow r(\\theta x + (1 - \\theta)y) \\leq l\\theta x + r(1 - \\theta)y\\\\\n",
    "&\\Rightarrow& r \\theta x \\leq l \\theta x \\Rightarrow l \\leq r \\quad (\\text{since } x < 0)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Therefore, under condition\n",
    "\n",
    "$$l \\leq r$$\n",
    "\n",
    "function $\\phi$ is convex, then $f$ is convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have,\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "f_1(x) = q(x) + l\\,x = a\\,x^2+b\\,x+c + l\\,x \\quad (x < 0)\\\\\n",
    "f_2(x) = q(x) + r\\,x = a\\,x^2+b\\,x+c + r\\,x \\quad (x \\geq 0)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Take the first derivative and let it be zero, we have,\n",
    "\\begin{eqnarray}\n",
    "x_1^* = -\\frac{b+l}{2a} \\\\\n",
    "x_2^* = -\\frac{b+r}{2a}\n",
    "\\end{eqnarray}\n",
    "\n",
    "From condition $l \\leq r$, we have,\n",
    "$$ -l \\geq -r $$ and $$ x_2^* \\leq x_1^* $$\n",
    "\n",
    "When $x_2^* \\ge 0$, which is $ b \\leq -r$, $x^* = x_2^* = -\\frac{b+r}{2a}$\n",
    "\n",
    "When $x_1^* < 0$, which is $ b > -l $, $x^* = x_1^* = -\\frac{b+l}{2a}$\n",
    "\n",
    "When $x_1^* \\ge 0$ && $x_2^* < 0$, which is $-r < b \\leq -l$, $x^* = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate descent is based on the idea that the minimization of a multivariable function $F(\\mathbf{x})$ can be achieved by minimizing it along one direction at a time, i.e., solving univariate (or at least much simpler) optimization problems in a loop.\n",
    "\n",
    "We can write down the lasso regression objective function as the following,\n",
    "\n",
    "$$|y - X\\,\\beta|^2 + \\lambda\\sum_{j = 1}^p |\\beta_j|$$\n",
    "\n",
    "Suppose we are solveing the $\\beta_i$, rearrange the objective function,\n",
    "\n",
    "$$|y - X\\,\\beta|^2 + \\lambda\\sum_{j = 1}^p |\\beta_j| = |y_i - x_i\\,\\beta_i|^2 + \\lambda\\sum_{j = 1, i \\neq j}^p |\\beta_j| + \\lambda |\\beta_i|$$\n",
    "\n",
    "where $y_i = y - [x_1, x_2, \\cdots, x_{i - 1}, x_{i + 1}\\cdots, x_p ]\\,[\\beta_1, \\beta_2, \\cdots \\beta_{i - 1}, \\beta_{i + 1}\\cdots \\beta_p]^\\top$\n",
    "\n",
    "Expand the rearranged objective function,\n",
    "\n",
    "$$\n",
    "|y_i - x_i\\,\\beta_i|^2 + \\lambda\\sum_{j = 1, i \\neq j}^p |\\beta_j| + \\lambda |\\beta_i| = |x_i|^2\\beta_i^2 - 2\\,(y_i \\cdot x_i)\\beta + |y_i|^2 + \\lambda\\sum_{j = 1, i \\neq j}^p |\\beta_j| + \\lambda |\\beta_i|\n",
    "$$\n",
    "\n",
    "where $a = |x_i|^2, b = - 2\\,(y_i \\cdot x_i), c = |y_i|^2 + \\lambda\\sum_{j = 1, i \\neq j}^p |\\beta_j|, l = -\\lambda, r = \\lambda$, so the critical part of the optimization of lasso becomes similar as the result we have in part (b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudocode,\n",
    "\n",
    "Given initial guess $x^0$ vector\n",
    "\n",
    "err = 1e-6\n",
    "\n",
    "f_old = inf\n",
    "\n",
    "f_new = 0\n",
    "\n",
    "k = 0\n",
    "\n",
    "while f_old - f_new > err    \n",
    "    \n",
    "    f_old = f_new \n",
    "    \n",
    "    for i = 1:n \n",
    "        \n",
    "        $\\text{Using part (b) trick to optimize}$\n",
    "        $x_i^{k+1} = \\text{argmin}_y f(x_1^{k+1}, x_2^{k+1} ... x_{i-1}^{k+1}, y, x_{i+1}^{k} ... x_n^{k})$ \n",
    "        \n",
    "    end\n",
    "    \n",
    "    $f_new = f(x^{k+1})$\n",
    "    \n",
    "    k = k + 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to maximize our final wealth, so the problem becomes, \n",
    "\n",
    "$$\\text{argmin}_{h}\\{\\frac{\\kappa}2h'\\Sigma\\,h - h'\\alpha + \\sum_j\\frac12 s_j|h_j - h_{0, j}|\\}$$\n",
    "\n",
    "Let $t = h - h_0$. And we have\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "&&\\frac{\\kappa}2 h'\\Sigma\\,h - h'\\alpha + \\sum_i\\frac12 s_i|h_i - h_{0, i}| \\\\\n",
    "&=& \\frac{\\kappa}2(h_0 + t)'\\Sigma\\,(h_0 + t) - (h_0 + t)'\\alpha + \\sum_j\\frac12 s_j|t_j|\\\\\n",
    "&=& \\frac{\\kappa}2 t'\\Sigma t + (\\kappa h_0' \\Sigma - \\alpha')t + (\\frac{\\kappa}2 h_0'\\Sigma h_0 - h_0'\\alpha) + \\sum_j\\frac12 s_j|t_j|\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We are optimizing sub problem $i_{th}$ holdings, we use notation $k$ for $k_{th}$ row and $j$ for $j_{th}$ column for the following derivation\n",
    "\n",
    "$$\n",
    "\\frac{\\kappa}2 t'\\Sigma\\,t = \\frac{\\kappa}2\\,\\sum_{k \\neq i,\\,j \\neq i}\\,t_k\\,\\Sigma_{kj}\\,t_j + (\\kappa\\sum_{k=i,\\,j \\neq i}\\,\\Sigma_{kj}\\,t_j)\\,t_i + \\frac{\\kappa}2\\,\\Sigma_{ii}\\,t_i^2 \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\kappa h_0' \\Sigma - \\alpha')t = \\sum_{k \\neq i}\\,(\\kappa h_0' \\Sigma - \\alpha')_k\\,t_k + (\\kappa h_0' \\Sigma - \\alpha')_i\\,t_i\n",
    "$$\n",
    "\n",
    "Therefore, the optimization problem becomes,\n",
    "\\begin{eqnarray*}\n",
    "&&\\frac{\\kappa}2 h'\\Sigma\\,h - h'\\alpha + \\sum_i\\frac12 s_i|h_i - h_{0, i}| \\\\\n",
    "&=& \\frac{\\kappa}2\\,\\sum_{k \\neq i,\\,j \\neq i}\\,t_k\\,\\Sigma_{kj}\\,t_j + (\\kappa\\sum_{k=i,\\,j \\neq i}\\,\\Sigma_{kj}\\,t_j)\\,t_i + \\frac{\\kappa}2\\,\\Sigma_{ii}\\,t_i^2 + \\sum_{k \\neq i}\\,(\\kappa h_0' \\Sigma - \\alpha')_k\\,t_k + (\\kappa h_0' \\Sigma - \\alpha')_i\\,t_i + (\\frac{\\kappa}2 h_0'\\Sigma h_0 - h_0'\\alpha) + \\sum_j\\frac12 s_j|t_j| \\\\\n",
    "&=& \\frac{\\kappa}2\\,\\Sigma_{ii}\\,t_i^2 + (\\kappa\\sum_{k=i,\\,j \\neq i}\\,\\Sigma_{kj}\\,t_j + (\\kappa h_0' \\Sigma - \\alpha')_i)\\,t_i + (\\frac{\\kappa}2\\,\\sum_{k \\neq i,\\,j \\neq i}\\,t_k\\,\\Sigma_{kj}\\,t_j + \\sum_{k \\neq i}\\,(\\kappa h_0' \\Sigma - \\alpha')_k\\,t_k + (\\frac{\\kappa}2 h_0'\\Sigma h_0 - h_0'\\alpha) + \\sum_{j \\neq i}\\frac12 s_j|t_j|) + \\frac12 s_i|t_i| \\\\\n",
    "&=& a\\,t_i^2 + b\\,t_i + c + \\lambda|t_i|\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where\n",
    "\\begin{eqnarray}\n",
    "a &=& \\frac{\\kappa}2\\,\\Sigma_{ii} \\\\\n",
    "b &=& (\\kappa\\sum_{k=i,\\,j \\neq i}\\,\\Sigma_{kj}\\,t_j + (\\kappa h_0' \\Sigma - \\alpha')_i) \\\\\n",
    "c &=& (\\frac{\\kappa}2\\,\\sum_{k \\neq i,\\,j \\neq i}\\,t_k\\,\\Sigma_{kj}\\,t_j + \\sum_{k \\neq i}\\,(\\kappa h_0' \\Sigma - \\alpha')_k\\,t_k + (\\frac{\\kappa}2 h_0'\\Sigma h_0 - h_0'\\alpha) + \\sum_{j \\neq i}\\frac12 s_j|t_j|) \\\\\n",
    "\\lambda &=& \\frac12 s_i \n",
    "\\end{eqnarray}\n",
    "\n",
    "The convex function becomes,\n",
    "\n",
    "$$f(t_i) = a\\,t_i^2 + b\\,t_i + c + \\lambda |t_i|$$\n",
    "\n",
    "Its closed form solution is,\n",
    "\n",
    "$$\n",
    "\\text{argmin} f(t_i) = \\left\\{ \\begin{array}{rl}\n",
    " -\\frac{b - \\lambda}{2\\,a} &\\mbox{ if $b \\geq \\lambda$} \\\\\n",
    "  0 &\\mbox{ if $-\\lambda < b \\leq \\lambda$} \\\\\n",
    "  -\\frac{b + \\lambda}{2\\,a} &\\mbox{ otherwise}\n",
    "       \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Therefore, the optimum $h_i^* = h_{0, i} + \\text{argmin} f(t_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trade happens three times, denote the profit $\\pi$, we have,\n",
    "\n",
    "$$\n",
    "\\pi = h_1\\,R_1 + h_2\\,R_2 - \\frac12\\,s(|h_1 - h_0| + |h_2 - h_1| + |h_2|) - b\\,h_2\\mathbb{1}_{h_2 < 0}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "w = w_0 + \\pi = w_0 + h_1\\,R_1 + h_2\\,R_2 - \\frac12\\,s(|h_1 - h_0| + |h_2 - h_1| + |h_2|) - b\\,h_2\\mathbb{1}_{h_2 < 0}\n",
    "$$\n",
    "\n",
    "So, we have,\n",
    "\n",
    "$$ E[w] = w_0 + h_1\\,\\mu_1 + h_2\\,\\mu_2 - \\frac12\\,s(|h_1 - h_0| + |h_2 - h_1| + |h_2|) - b\\,h_2\\mathbb{1}_{h_2 < 0} $$\n",
    "\n",
    "Without lossing generality, we assume $R_1$ and $R_2$ are uncorrelated, so we have\n",
    "$$\n",
    "V[w] = \\sigma^2 (h_1^2 + h_2^2)\n",
    "$$\n",
    "\n",
    "Therefore\n",
    "$$\n",
    "u(h_1, h_2) = \\mathbb{E}[w] - \\frac{\\kappa}2 V[w] = w_0 + h_1\\,\\mu_1 + h_2\\,\\mu_2 - \\frac12\\,s(|h_1 - h_0| + |h_2 - h_1| + |h_2|) - b\\,h_2\\mathbb{1}_{h_2 < 0} - \\frac{\\kappa}2 \\sigma^2 (h_1^2 + h_2^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a quadratic function but the coefficient for the quadratic term, $-\\frac{\\kappa}2\\,\\sigma^2$, is negative. So it is not a convex function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
